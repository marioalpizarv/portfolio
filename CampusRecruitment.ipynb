{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a7475606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "\n",
    "#!pip install seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b7716b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''\n",
    "file = 'Placement_Data_Full_Class.csv'\n",
    "\n",
    "df = pd.read_csv(path + file)\n",
    "df = df.dropna()\n",
    "\n",
    "#array of categorical variables to encode\n",
    "#prefix=['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation', 'status'] \n",
    "\n",
    "# Perform one-hot encoding, add columns for each categorical variable\n",
    "#df_encoded = pd.get_dummies(df, prefix)\n",
    "\n",
    "#print(df)\n",
    "#print(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f88ff31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sl_no  ssc_p  hsc_p  degree_p  etest_p  mba_p    salary  gender_F  \\\n",
      "0        1  67.00  91.00     58.00     55.0  58.80  270000.0         0   \n",
      "1        2  79.33  78.33     77.48     86.5  66.28  200000.0         0   \n",
      "2        3  65.00  68.00     64.00     75.0  57.80  250000.0         0   \n",
      "4        5  85.80  73.60     73.30     96.8  55.50  425000.0         0   \n",
      "7        8  82.00  64.00     66.00     67.0  62.14  252000.0         0   \n",
      "..     ...    ...    ...       ...      ...    ...       ...       ...   \n",
      "209    210  62.00  72.00     65.00     67.0  56.49  216000.0         0   \n",
      "210    211  80.60  82.00     77.60     91.0  74.49  400000.0         0   \n",
      "211    212  58.00  60.00     72.00     74.0  53.62  275000.0         0   \n",
      "212    213  67.00  67.00     73.00     59.0  69.72  295000.0         0   \n",
      "213    214  74.00  66.00     58.00     70.0  60.23  204000.0         1   \n",
      "\n",
      "     gender_M  ssc_b_Central  ...  hsc_b_Central  hsc_b_Others  hsc_s_Arts  \\\n",
      "0           1              0  ...              0             1           0   \n",
      "1           1              1  ...              0             1           0   \n",
      "2           1              1  ...              1             0           1   \n",
      "4           1              1  ...              1             0           0   \n",
      "7           1              1  ...              1             0           0   \n",
      "..        ...            ...  ...            ...           ...         ...   \n",
      "209         1              1  ...              1             0           0   \n",
      "210         1              0  ...              0             1           0   \n",
      "211         1              0  ...              0             1           0   \n",
      "212         1              0  ...              0             1           0   \n",
      "213         0              0  ...              0             1           0   \n",
      "\n",
      "     hsc_s_Commerce  hsc_s_Science  workex_No  workex_Yes  \\\n",
      "0                 1              0          1           0   \n",
      "1                 0              1          0           1   \n",
      "2                 0              0          1           0   \n",
      "4                 1              0          1           0   \n",
      "7                 0              1          0           1   \n",
      "..              ...            ...        ...         ...   \n",
      "209               1              0          1           0   \n",
      "210               1              0          1           0   \n",
      "211               0              1          1           0   \n",
      "212               1              0          0           1   \n",
      "213               1              0          1           0   \n",
      "\n",
      "     specialisation_Mkt&Fin  specialisation_Mkt&HR  status_Placed  \n",
      "0                         0                      1              1  \n",
      "1                         1                      0              1  \n",
      "2                         1                      0              1  \n",
      "4                         1                      0              1  \n",
      "7                         1                      0              1  \n",
      "..                      ...                    ...            ...  \n",
      "209                       1                      0              1  \n",
      "210                       1                      0              1  \n",
      "211                       1                      0              1  \n",
      "212                       1                      0              1  \n",
      "213                       0                      1              1  \n",
      "\n",
      "[148 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['degree_t'], axis=1)\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Select the dependent variable\n",
    "y = df['degree_t'] \n",
    "y = pd.get_dummies(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a51ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63f60385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created our first MLP network\n"
     ]
    }
   ],
   "source": [
    "# This creates our first MLP with 1 hidden layer with 50 neurons and sets it to run through the data 20 times\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(200,200, 200), max_iter=20, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "print(\"Created our first MLP network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bba50ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 72.08749558\n",
      "Iteration 2, loss = 2631.21794039\n",
      "Iteration 3, loss = 25655475483455539716993056768.00000000\n",
      "Iteration 4, loss = 52192894085851310905588973568.00000000\n",
      "Iteration 5, loss = 3505528810561617998619094192118316963794501238784.00000000\n",
      "Iteration 6, loss = 12372671650901645121667824553493385523754052878336.00000000\n",
      "Iteration 7, loss = 22146840863710001170035428020731813492877336510464.00000000\n",
      "Iteration 8, loss = 33454903222299944681584687821651904164943682863104.00000000\n",
      "Iteration 9, loss = 45666315541160797978379184178286698780546918514688.00000000\n",
      "Iteration 10, loss = 58304247625113982304904653901532217308511826083840.00000000\n",
      "Iteration 11, loss = 71012985932073366119562970543242085260115000164352.00000000\n",
      "Iteration 12, loss = 83531869605763824821579974031428125090560266469376.00000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.661017\n",
      "Test set score: 0.800000\n"
     ]
    }
   ],
   "source": [
    "#train the NN\n",
    "mlp1.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp1.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19d97ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "     sl_no  ssc_p  hsc_p  degree_p  etest_p  mba_p    salary  gender_F  \\\n",
      "178    179  68.00  56.00     68.00    73.00  68.07  350000.0         0   \n",
      "74      75  56.60  64.80     70.20    84.27  67.20  336000.0         0   \n",
      "203    204  55.68  61.33     56.87    66.00  58.30  260000.0         0   \n",
      "28      29  76.76  76.50     67.50    73.35  64.15  350000.0         0   \n",
      "145    146  89.40  65.66     71.25    72.00  63.23  400000.0         0   \n",
      "20      21  62.00  65.00     66.00    50.00  56.70  265000.0         0   \n",
      "112    113  58.00  61.00     61.00    58.00  53.94  250000.0         0   \n",
      "48      49  63.00  62.00     68.00    64.00  62.46  250000.0         0   \n",
      "117    118  77.00  75.00     73.00    80.00  67.05  240000.0         0   \n",
      "15      16  65.00  75.00     69.00    72.00  64.66  200000.0         1   \n",
      "39      40  81.00  68.00     64.00    93.00  62.56  411000.0         0   \n",
      "134    135  77.44  92.00     72.00    94.00  67.13  250000.0         1   \n",
      "210    211  80.60  82.00     77.60    91.00  74.49  400000.0         0   \n",
      "96      97  76.00  70.00     76.00    66.00  64.44  300000.0         1   \n",
      "197    198  83.96  53.00     91.00    59.32  69.71  260000.0         1   \n",
      "95      96  73.00  78.00     65.00    95.46  62.16  420000.0         0   \n",
      "27      28  63.00  67.00     66.00    68.00  57.69  265000.0         0   \n",
      "101    102  63.00  72.00     68.00    78.00  60.44  380000.0         0   \n",
      "177    178  73.00  97.00     79.00    89.00  70.81  650000.0         1   \n",
      "47      48  63.00  60.00     57.00    78.00  54.55  204000.0         0   \n",
      "44      45  77.00  73.00     81.00    89.00  69.70  200000.0         1   \n",
      "146    147  62.00  63.00     66.00    85.00  55.14  233000.0         0   \n",
      "56      57  63.00  71.40     61.40    68.00  66.88  240000.0         0   \n",
      "164    165  67.16  72.50     63.35    53.04  65.52  250000.0         1   \n",
      "80      81  69.00  62.00     69.00    67.00  62.35  240000.0         1   \n",
      "33      34  87.00  65.00     81.00    88.00  72.78  260000.0         1   \n",
      "92      93  60.23  69.00     66.00    72.00  59.47  230000.0         1   \n",
      "192    193  65.20  61.40     64.80    93.40  57.34  270000.0         0   \n",
      "118    119  76.00  80.00     78.00    97.00  70.48  276000.0         0   \n",
      "19      20  60.00  67.00     70.00    50.48  77.89  236000.0         0   \n",
      "\n",
      "     gender_M  ssc_b_Central  ...  hsc_b_Central  hsc_b_Others  hsc_s_Arts  \\\n",
      "178         1              0  ...              0             1           0   \n",
      "74          1              1  ...              1             0           0   \n",
      "203         1              0  ...              0             1           0   \n",
      "28          1              0  ...              0             1           0   \n",
      "145         1              0  ...              0             1           0   \n",
      "20          1              0  ...              0             1           0   \n",
      "112         1              0  ...              0             1           0   \n",
      "48          1              0  ...              0             1           0   \n",
      "117         1              0  ...              0             1           0   \n",
      "15          0              1  ...              1             0           0   \n",
      "39          1              0  ...              0             1           0   \n",
      "134         0              1  ...              0             1           0   \n",
      "210         1              0  ...              0             1           0   \n",
      "96          0              1  ...              1             0           0   \n",
      "197         0              0  ...              0             1           0   \n",
      "95          1              1  ...              0             1           0   \n",
      "27          1              0  ...              0             1           0   \n",
      "101         1              1  ...              1             0           0   \n",
      "177         0              1  ...              0             1           0   \n",
      "47          1              1  ...              1             0           0   \n",
      "44          0              0  ...              0             1           0   \n",
      "146         1              1  ...              0             1           0   \n",
      "56          1              0  ...              0             1           0   \n",
      "164         0              1  ...              1             0           0   \n",
      "80          0              0  ...              0             1           0   \n",
      "33          0              0  ...              0             1           0   \n",
      "92          0              1  ...              1             0           0   \n",
      "192         1              1  ...              1             0           0   \n",
      "118         1              1  ...              1             0           0   \n",
      "19          1              0  ...              0             1           1   \n",
      "\n",
      "     hsc_s_Commerce  hsc_s_Science  workex_No  workex_Yes  \\\n",
      "178               0              1          1           0   \n",
      "74                1              0          1           0   \n",
      "203               1              0          1           0   \n",
      "28                1              0          0           1   \n",
      "145               0              1          1           0   \n",
      "20                1              0          1           0   \n",
      "112               1              0          1           0   \n",
      "48                1              0          1           0   \n",
      "117               0              1          1           0   \n",
      "15                1              0          0           1   \n",
      "39                0              1          1           0   \n",
      "134               1              0          0           1   \n",
      "210               1              0          1           0   \n",
      "96                0              1          0           1   \n",
      "197               0              1          1           0   \n",
      "95                1              0          0           1   \n",
      "27                1              0          1           0   \n",
      "101               1              0          1           0   \n",
      "177               1              0          0           1   \n",
      "47                1              0          0           1   \n",
      "44                1              0          0           1   \n",
      "146               0              1          1           0   \n",
      "56                1              0          1           0   \n",
      "164               1              0          1           0   \n",
      "80                1              0          0           1   \n",
      "33                0              1          0           1   \n",
      "92                0              1          1           0   \n",
      "192               1              0          0           1   \n",
      "118               0              1          0           1   \n",
      "19                0              0          0           1   \n",
      "\n",
      "     specialisation_Mkt&Fin  specialisation_Mkt&HR  status_Placed  \n",
      "178                       0                      1              1  \n",
      "74                        1                      0              1  \n",
      "203                       0                      1              1  \n",
      "28                        1                      0              1  \n",
      "145                       0                      1              1  \n",
      "20                        0                      1              1  \n",
      "112                       0                      1              1  \n",
      "48                        1                      0              1  \n",
      "117                       1                      0              1  \n",
      "15                        1                      0              1  \n",
      "39                        1                      0              1  \n",
      "134                       1                      0              1  \n",
      "210                       1                      0              1  \n",
      "96                        1                      0              1  \n",
      "197                       0                      1              1  \n",
      "95                        1                      0              1  \n",
      "27                        0                      1              1  \n",
      "101                       0                      1              1  \n",
      "177                       1                      0              1  \n",
      "47                        1                      0              1  \n",
      "44                        1                      0              1  \n",
      "146                       0                      1              1  \n",
      "56                        1                      0              1  \n",
      "164                       1                      0              1  \n",
      "80                        0                      1              1  \n",
      "33                        1                      0              1  \n",
      "92                        1                      0              1  \n",
      "192                       1                      0              1  \n",
      "118                       0                      1              1  \n",
      "19                        1                      0              1  \n",
      "\n",
      "[30 rows x 21 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_test)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Now let's visualize the errors between the predictions and the actual labels using a confusion matrix\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mmatshow(cm)\n",
      "File \u001b[0;32m~/Documents/notebook/jupyter_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:309\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    307\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "\u001b[0;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "# STEP 3.2\n",
    "\n",
    "# First let's initialize a list with all the predicted values from the training set\n",
    "y_pred = mlp1.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "print(X_test)\n",
    "\n",
    "# Now let's visualize the errors between the predictions and the actual labels using a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.matshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6e33e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 584ms/step - loss: 2.0540 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.9224 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.7917 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6655 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5458 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4343 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3327 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2420 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1657 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1061 - accuracy: 0.5000\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2b8aba8550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.6803 - accuracy: 0.0000e+00\n",
      "Test loss: 1.6803014278411865\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load your data with multiple variables into a DataFrame\n",
    "data = {\n",
    "    'Variable1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'Variable2': [5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'Variable3': [9, 10, 11, 12, 13, 14, 15, 16],\n",
    "    'Variable4': [13, 14, 15, 16, 17, 18, 19, 20],\n",
    "    'Target': ['ClassA', 'ClassB', 'ClassC', 'ClassA', 'ClassB', 'ClassC', 'ClassA', 'ClassB']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the dataset into input features (X) and target variable (y)\n",
    "X = df.drop('Target', axis=1)  # Exclude the target variable\n",
    "y = df['Target']\n",
    "\n",
    "# One-hot encode the categorical target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(X.shape[1],)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(y_encoded.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fad4d0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "[[0.7013209  0.16744353 0.13123554]\n",
      " [0.5839777  0.20870276 0.20731962]]\n",
      "   ClassA  ClassB  ClassC\n",
      "1       0       1       0\n",
      "5       0       0       1\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Now let's visualize the errors between the predictions and the actual labels using a confusion matrix\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mmatshow(cm)\n",
      "File \u001b[0;32m~/Documents/notebook/jupyter_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:309\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    307\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "\u001b[0;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "# First let's initialize a list with all the predicted values from the training set\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)\n",
    "y_pred = y_pred.astype(int)\n",
    "print(y_test)\n",
    "print(y_pred)\n",
    "\n",
    "# Now let's visualize the errors between the predictions and the actual labels using a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.matshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a00beb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 00:17:43.853952: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: logits and labels must have the same first dimension, got logits shape [32,1] and labels shape [96]\n",
      "\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n      await self.process_one()\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 450, in process_one\n      await dispatch(*args)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n      await result\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n      reply_content = await reply_content\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2768, in run_cell\n      result = self._run_cell(\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2814, in _run_cell\n      return runner(coro)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3012, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3251, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_7373/1912084938.py\", line 12, in <module>\n      model.fit(X, y, epochs=10, batch_size=32)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [32,1] and labels shape [96]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_1159]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[0;32m~/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/notebook/jupyter_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n      await self.process_one()\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 450, in process_one\n      await dispatch(*args)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n      await result\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n      reply_content = await reply_content\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2768, in run_cell\n      result = self._run_cell(\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2814, in _run_cell\n      return runner(coro)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3012, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3251, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_7373/1912084938.py\", line 12, in <module>\n      model.fit(X, y, epochs=10, batch_size=32)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/malpizar/Documents/notebook/jupyter_env/lib/python3.8/site-packages/keras/backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [32,1] and labels shape [96]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_1159]"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape=(21,)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print the predicted classes\n",
    "predicted_classes = tf.argmax(predictions, axis=1)\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67006dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
